# -*- coding: utf-8 -*-
"""1.chunking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8yf6yS5Z6AdO3pkqYwLycEnKxIOI46s

# CodingMindset Lab - Langchain - RAG 102
"""



"""## Prerrequisitos

1. Instalamos las dependencias necesarias
"""

#!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas faiss-cpu tiktoken

"""2. Seteamos la API Key de OpenAI, si no lo hemos hecho previamente"""

import os
import getpass

os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API Key:")

"""## Descarga de la base de conocimiento

> NOTA: Podemos utilizar, cualquier documento detexto o pdf que tengamos a

*   Elemento de lista
*   Elemento de lista

nuestro alcance. En caso contrario podemos usar la web the Gutenberg.org para descargar cualquier libro disponible gratuitamente

- Descargamos un libro sobre el funcionamiento del cerebro en formato txt y le damos el nombre de "the_brain"
"""

!wget https://gutenberg.org/cache/epub/14586/pg14586.txt -O the_brain.txt

with open("./the_brain.txt") as f:
  the_brain = f.read()

"""## Estrategias de Chunking

### Recursive Splitting

También conocido como "Naive chunking", suele ser la estrategia usada por defecto en LangChain en la mayoría de tutoriales o para el enfoque "Naive RAG"<br> Utiliza reglas sintácticas.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=0,
    length_function=len,
)

naive_chunks = text_splitter.split_text(the_brain)

for chunk in naive_chunks[40:55]:
  print(chunk + "\n")

"""### Semantic Chunk

El chunking semántico implica dividir el texto en segmentos cohesivos y significativos basados en el contenido semántico. Este método asegura que cada fragmento represente un tema o idea coherente, lo cual mejora la relevancia y el contexto durante los procesos de recuperación y generación de información.

1. Inicializamos la clase, pasándole por parámetro el embedding model que vamos a usar
"""

from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

semantic_chunker = SemanticChunker(OpenAIEmbeddings(model="text-embedding-3-large"), breakpoint_threshold_type="percentile")

"""El segundo parámetro dispone de tres métodos comunes para realizar el chunking semántico, según la [documentación de LangChain](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker):

- **Método del Percentil (`percentile`)**: Se calculan todas las diferencias entre frases y cualquier diferencia mayor al percentil X se utiliza para dividir.
- **Método de la Desviación Estándar (`standard_deviation`)**: Divide en puntos donde las diferencias superen X desviaciones estándar.
- **Método Intercuartil (`interquartile`)**: Utiliza la distancia intercuartil para determinar los puntos de división.

#### Pasos Básicos:

1. **División del Documento:** Fragmenta el texto en frases basándose en signos de puntuación terminativos como `.`, `?`, y `!`.
2. **Indexación de Frases:** Asigna un índice a cada frase según su posición en el texto.
3. **Buffering:** Añade un `buffer_size` (int) de frases alrededor de cada sentencia clave para contextualizar.
4. **Cálculo de Distancias:** Mide las distancias semánticas entre los grupos de frases.
5. **Unión Basada en Similitud:** Fusiona los grupos de oraciones que sean similares utilizando los límites establecidos por los métodos de chunking.

> **Nota:** Este método es experimental y podría recibir mejoras y actualizaciones en el futuro próximo.

2. Creamos los chunks
"""

semantic_chunks = semantic_chunker.create_documents([the_brain])

for semantic_chunk in semantic_chunks:
  if "MDT is associated with the basic" in semantic_chunk.page_content:
    print(semantic_chunk.page_content)
    print(len(semantic_chunk.page_content))

"""## RAG APP

Vamos a crear una RAG app, usando nuestra nueva estrategia de chunking con LangChain usando LCEL

### Retrieval

En esta ocasión usaremos **FAISS (Facebook AI Similarity Search)**. la cual es una biblioteca optimizada para realizar búsquedas rápidas en grandes bases de datos de vectores almacenados en memoria, ideal para tareas de recuperación de información y sistemas de recomendación debido a su alta velocidad y escalabilidad.
"""

from langchain_community.vectorstores import FAISS

semantic_chunk_vectorstore = FAISS.from_documents(semantic_chunks, embedding=OpenAIEmbeddings(model="text-embedding-3-large"))

"""Limitaremos (`semantic_chunk_vectorstore`) a k = 1 para demostrar el poder de la estrategia de chunking semántico, manteniendo un conteo de tokens similar entre el contexto recuperado semánticamente y el contexto recuperado de manera simple."""

semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={"k" : 1})

semantic_chunk_retriever.invoke("what is MDT?")

"""### Augmented

- Descargamos el prompt desde ek langchain hub por comodidad, pero podriamos crear neustro propio prompt de la siguiente manera:
````python
from langchain_core.prompts import ChatPromptTemplate

rag_template = '''\
Use the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.

User's Query:
{question}

Context:
{context}
'''

rag_prompt = ChatPromptTemplate.from_template(rag_template)

````
"""

from langchain import hub

prompt = hub.pull("rlm/rag-prompt")

"""### Generation

- Utilizaremos `ChatOpenAI()` para mantener la simplicidad del ejemplo
"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

"""### LCEL RAG Chain"""

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

semantic_rag_chain = (
    {"context" : semantic_chunk_retriever, "question" : RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

semantic_rag_chain.invoke("what is MDT?")

